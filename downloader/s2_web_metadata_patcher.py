import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import time

# =============================================================================
# SCRIPT: Semantic Scholar Metadata Recovery Patch
# PURPOSE: This patch targets rows where the API failed to return an abstract,
#          but the content is visible on the S2 web frontend.
# STRATEGY: It bypasses keyword matching and extracts structured JSON-LD data
#           directly from the Document Object Model (DOM).
# =============================================================================

def scrape_s2_json_ld(url):
    """
    Retrieves the paper abstract from the structured JSON-LD block in the HTML.
    This method is highly robust against non-standard page layouts.
    """
    if pd.isna(url) or 'semanticscholar.org' not in str(url):
        return None

    # Use a realistic User-Agent to emulate a standard browser and prevent 403 blocks
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
    }

    try:
        # Fetch the HTML content from the Semantic Scholar URL
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # Locate the JSON-LD script tag which contains structured metadata
        # Even if the word 'Abstract' is missing from the UI, it exists here for SEO
        json_scripts = soup.find_all('script', type='application/ld+json')
        
        for script in json_scripts:
            try:
                data = json.loads(script.text)
                
                # Check for 'description' or 'headline' fields in the JSON object
                # Most academic portals use 'description' for the abstract content
                abstract = data.get('description') or data.get('headline')
                
                if abstract and len(str(abstract)) > 50:
                    return str(abstract)
            except (json.JSONDecodeError, AttributeError):
                continue

        # Secondary Fallback: Extract from OpenGraph or standard Meta tags
        meta_tags = ["og:description", "description"]
        for tag_name in meta_tags:
            meta = soup.find("meta", {"name": tag_name}) or soup.find("meta", {"property": tag_name})
            if meta and len(meta.get('content', '')) > 50:
                return meta['content']

    except Exception as e:
        print(f"      [Exception] Failed to scrape {url}: {e}")
        return None
    return None

def main():
    # Load the latest dataset generated by the API patch
    input_file = 'pdf_with_abstracts_final.csv' 
    df = pd.read_csv(input_file)
    
    # Identify entries still marked as missing
    mask = (df['Abstract'] == "ABSTRACT_MISSING_FALLBACK_TO_TITLE") | (df['Abstract'].isna())
    missing_indices = df[mask].index
    
    print(f"--- Starting Tier-2 Web Recovery Patch for {len(missing_indices)} entries ---")

    for idx, i in enumerate(missing_indices):
        s2_url = str(df.at[i, 'Semantic Scholar URL'])
        print(f"[{idx+1}/{len(missing_indices)}] Processing Page: {s2_url}")
        
        # Execute the deep metadata extraction
        recovered_text = scrape_s2_json_ld(s2_url)
        
        if recovered_text:
            # Clean up boilerplate text prepended by some web scrapers/metatags
            clean_text = recovered_text.replace("Semantic Scholar extracted view of ", "").strip()
            df.at[i, 'Abstract'] = clean_text
            print("      Result: [SUCCESS] Abstract recovered from metadata.")
        else:
            print("      Result: [FAIL] Metadata block not found.")
        
        # Throttling requests to respect S2's crawling policies and avoid IP blacklisting
        time.sleep(3.0)

    # Export the high-fidelity final dataset
    output_file = 'pdf_with_abstracts_S2_ULTIMATE.csv'
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\n--- Data Enrichment Complete. Final File: {output_file} ---")

if __name__ == "__main__":
    main()